{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Problem Sheet\n",
    "\n",
    "Solutions to the Tensorflow Problem Sheet as part of Emerging Technologies Module - 4th Year Software Development\n",
    " \n",
    "#### Reference: https://github.com/emerging-technologies/keras-iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Use Tensorflow to create model\n",
    "\n",
    "#### Use Tensorflow to create a model to predict the species of Iris from a flower's sepal width, sepal length, petal width, and petal length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we get all needed imports and then load in the [Iris Dataset](https://github.com/mwaskom/seaborn-data/blob/master/iris.csv) into a list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Adapted from: https://github.com/salmanahmad4u/keras-iris/blob/master/iris_nn.py\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import keras as kr\n",
    "\n",
    "# Loading in the Iris dataset\n",
    "iris = list(csv.reader(open('iris.csv')))[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to split the dataset into float Arrays of Sepal Length, sepal width, petal length and petal with.\n",
    "<br>\n",
    "We do this using the 'Slicing' Technique. Slicing is is used when you want to seperate a list/array etc into sub-elements.\n",
    "<br>\n",
    "Reference: http://www.pythoncentral.io/how-to-slice-listsarrays-and-tuples-in-python/\n",
    "<br>\n",
    "\n",
    "Then where we have the outputs: Setosa, Versocolor or Virginica\n",
    "<br>\n",
    "We want to convert these strings into ints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The inputs are four floats\n",
    "inpts  = np.array(iris)[:,:4].astype(np.float)\n",
    "\n",
    "# Outputs are initially individual strings\n",
    "out = np.array(iris)[:,4]\n",
    "# Converting the strings to ints.\n",
    "out_vals, out_ints = np.unique(out, return_inverse=True)\n",
    "# Encode the category integers as binary categorical vairables.\n",
    "out_cats = kr.utils.to_categorical(out_ints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split the data into training and testing\n",
    "\n",
    "#### Split the data set into a training set and a testing set. You should investigate the best way to do this, and list any online references used in your notebook. If you wish to, you can write some code to randomly separate the data on the fly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, we want to split the sets into Training and Test Subsets. These are created in order to train the model we will soon create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inds = np.random.permutation(len(inpts))\n",
    "train_inds, test_inds = np.array_split(inds, 2)\n",
    "in_train, out_train = inpts[train_inds], out_cats[train_inds]\n",
    "in_test,  out_test  = inpts[test_inds],  out_cats[test_inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the model\n",
    "\n",
    "#### Use the testing set to train your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, here we need to Create a neural network which we are using [KERAS](https://keras.io/) to do!\n",
    "\n",
    "Then we configure the model using the adam optimizer and the categorical cross entropy and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 1.2227 - acc: 0.3800\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 1.1060 - acc: 0.3800\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 1.0238 - acc: 0.3800\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.9741 - acc: 0.3800\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.9304 - acc: 0.4400\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.9009 - acc: 0.7000\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.8724 - acc: 0.7000\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.8368 - acc: 0.7000\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.8151 - acc: 0.7000\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.7865 - acc: 0.7000\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.7559 - acc: 0.7000\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.7373 - acc: 0.7400\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.7146 - acc: 0.7000\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.6862 - acc: 0.7000\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.6669 - acc: 0.7000\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.6528 - acc: 0.7200\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.6282 - acc: 0.7200\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.6077 - acc: 0.7200\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.5888 - acc: 0.7200\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.5752 - acc: 0.7800\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.5562 - acc: 0.7200\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.5387 - acc: 0.8000\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.5210 - acc: 0.8000\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.5081 - acc: 0.8000\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.5017 - acc: 0.7400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b9a7158470>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sequential neural network/model called m.\n",
    "m = kr.models.Sequential()\n",
    "\n",
    "# Add an initial layer with 4 input nodes, and a hidden layer with 16 nodes and then activate the sigmoid function\n",
    "m.add(kr.layers.Dense(16, input_shape=(4,)))\n",
    "m.add(kr.layers.Activation(\"sigmoid\"))\n",
    "\n",
    "# Adding another layer and activate the softmax function to that layer\n",
    "m.add(kr.layers.Dense(3))\n",
    "m.add(kr.layers.Activation(\"softmax\"))\n",
    "\n",
    "m.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Training data\n",
    "m.fit(in_train, out_train, epochs=25, batch_size=1, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test the model\n",
    "\n",
    "#### Use the testing set to test your model, clearly calculating and displaying the error rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 143us/step\n",
      "\n",
      "\n",
      "Loss:     0.5165\tAccuracy:     0.7551\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model using the test data set.\n",
    "loss, accuracy = m.evaluate(in_test, out_test, verbose=1)\n",
    "\n",
    "#Printing Loss & Accuracy\n",
    "print(\"\\n\\nLoss: %10.4f\\tAccuracy: %10.4f\" % (loss, accuracy))\n",
    "\n",
    "# Saving the model to a file\n",
    "m.save(\"iris_nn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
